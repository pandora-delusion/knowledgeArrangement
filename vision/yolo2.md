# YOLO9000和YOLOv2

当前对象检测数据集相比于分类和标注等其他任务的数据集相比是有限的，用于检测的标记图像比用于分类或标记的要昂贵的多。

YOLO9000 提出了一种新的方法利用已有的大量分类数据，并利用它来扩大现有检测系统的范围。该方法从对象分类的层次视角触发，将不同的数据集结合在一起。同时还提出了一种联合训练算法，能够在检测和分类数据上训练目标检测器。该方法利用标记检测图像来学习精确定位目标，同时使用分类图像来增加词汇量和鲁棒性。

在YOLO系统的基础上进行了改进，产生了实时监测系统YOLOv2。然后利用数据组合方法和联合训练方法对模型进行多目标训练，数据来自ImageNet的9000多个分类和来自COCO的检测数据。

YOLO和Fast R-CNN的误差分析表明，YOLO存在大量的定位误差，与基于region proposal的方法相比，YOLO的召回率相对较低。

### YOLO9000的各种新思想

#### Batch Normalization

BN层在收敛性方法有显著的改进，同时消除了对其他形式正则化的需要。通过在YOLO中对所有卷积层添加BN层，我们在mAP的结果中得到了2%以上的改进。BN层还有助于对模型进行正则化。通过BN层，可以替代dropout防止过拟合。

#### High Resolution Classifier

目前所有的检测方法都使用在ImageNet上预训练过的分类器。这意味着网络必须同时切换到学下对象检测和调整到新的输入分辨率。在YOLOv2上，首先在$448 \times 448$分辨率上在ImageNet上微调分类器网络10个epoch。这给网络时间来调整它的过滤器，以更好地工作在更高分辨率的输入上，在后再在检测网络上微调。这种高分辨率的微调使mAP增加了4%

#### Convolutional With Anchor Boxes

YOLO使用卷积特征提取器上的全连接层直接预测边框坐标。使用anchor box并且预测偏移量而不是坐标简化了问题，并且使得网络更加容易学习。移除掉YOLO的全连接层并使用anchor box开预测边框：消除一个池化层，是网络卷积层的输出具有更高的分辨率。在将网络缩小到416的输入像素而不是$448 \times 448$。这样做是因为我们想要在功能图中有奇数个位置，所以只有一个中心单元格。物体，尤其是大物体，往往占据图像的中心，所以最好在中心有一个位置来预测这些物体，而不是四个都在附近的位置。同时将空间位置与种类预测机制解耦，从而每个anchor box都有预测种类和边框回归。使用anchor box后，模型mAP略有下降但是召回率（recall）有上升。

#### Dimension Clusters

在YOLO上使用anchor box时，有两个问题。

第一个问题是box的尺寸是人工挑选的。网络可以学会适当的调整边框，但如果为网络选择更好的先验，我们可以使网络更容易学会预测好的检测。现在训练集边框集合中跑k-means聚类寻找好的先验。聚类算法总重要的使选择如何计算两个边框之间的距离，在这里我们使用边框的IoU。所以在聚类时采用以下公式来计算两个边框之间的距离：
$$
d(box, centroid)=1-IOU(box, centroid)
$$
论文中选择k=5作为模型复杂度和高回叫率的平衡。和手工选出的box不同，聚类选出的box多为瘦高型边框。使用k-means来生成边界框将使模型具有更好的表示，并使任务更容易学习。

#### Direct Location Prediction

在YOLO上使用anchor box的第二个问题是，模型的稳定性，尤其是在早期迭代期间。绝大多数的不稳定性来自于预测边框的（x，y）的位置。在RPN中预测的$t_{x}$、$t_{y}$计算如下：
$$
x=(t_{x}*w_{a})+x_{a}\\
y=(t_{y}*h_{a})+y_{a}
$$
这个公式是不受约束的，因此任何anchor box可以被定位在图像的任何位置，而不管anchor box预测的位置是什么。在随机初始化的情况下，该模型需要很长时间才能稳定地预测敏感偏移量。

在本文中，我们预测的位置坐标是相对于网格单元节点的位置。这将gt限制在0和1之间。我们使用logistic激活函数来限制网络的预测落在这个范围内。

网络在输出特征图上的每个单元上预测5个边框，每个边框预测5个坐标，$t_{x},t_{y},t_{w},t_{h},t_{o}$。若单元$(c_{x},c_{y})$是距离图片左上角的偏置，边框先验的宽和长为$P_{w},P_{h}$，那么对应的预测为：
$$
b_{x}=\sigma (t_{x})+c_{x}\\
b_{y}=\sigma (t_{y})+c_{y}\\
b_{w}=p_{w} e^{t_{w}}\\
b_{h}=p_{h} e^{t_{h}}\\
Pr(object)*IOU(b,object)=\sigma(t_{o}) 
$$
由于对位置预测进行了约束，使得参数化更容易学习，使得网络更加稳定。

#### Fine-Grained Features

改进的YOLO在$13 \times 13$的特征图上检测。为了解决YOLO很难精确定位小目标的问题，YOLOv2采用和SSD不同的方法，引入一种称为passthrough层的方法在特征图中保留一些细节信息。具体来说，就是在最后一个pooling之前，特征图的大小是26*26*512，将其1拆4，直接传递（passthrough）到pooling后（并且又经过一组卷积）的特征图，两者叠加到一起作为输出的特征图。

![](F:\mycode\knowledgeArrangement\vision\passthrough.jpg)

图中示例的是1个4*4拆成4个2*2。因为深度不变，所以没有画出来

![](F:\mycode\knowledgeArrangement\vision\1to4.jpg)

#### Multi-Scale Training

因为去掉了全连接层，YOLO2可以输入任何尺寸的图像。因为整个网络下采样倍数是32，作者采用了{320,352,...,608}等10种输入图像的尺寸。训练时每10个batch就随机更换一种尺寸，使网络能够适应各种大小的对象检测。这种机制迫使网络学会很好地预测各种输入维度。这意味着同一个网络可以预测不同分辨率下的检测。网络在较小的尺寸下运行得更快，因此YOLOv2在速度和精度之间提供了一个简单的折衷。

### DarkNet-19

一种新的分类模型作为YOLOv2的基础。类似于VGG网络主要应用$3 \times 3$的过滤器并在每次池化操作后加倍通道数。遵循[NiN](https://arxiv.org/pdf/1312.4400.pdf)的工作，使用使用全剧平均池化层和用$1 \times 1$过滤器在$3 \times 3$的卷积层之间压缩特征表示。使用BN层来稳定训练，加速收敛并对模型进行正则化。比VGG-16小一些，精度不弱于VGG-16，但是浮点运算量减少到约1/5，以保证更快的运算速度。

![](F:\mycode\knowledgeArrangement\vision\darknet19.png)

### 训练过程

​	论文提出了一种分类与检测数据联合训练的机制。该机制使用检测用的标记图像来学习检测专用的信息，比如边框坐标预测和如何分类一般目标；使用分类用的标记图像（只有分类标记的图像）来拓展系统能够检测的种类数量。在训练中混合来自检测和分类数据集的图像。**系统使得被标记为检测的图像以基于完整的YOLOv2损失函数进行反向传播，使被标记为分类的图像只在结构特定的分类部分上反向传播**

​	如果要用一种连贯的方法来合并这些标签来对这两个数据集进行训练。大多数分类方法在所有可能的类别中使用softmax层来计算最终的概率分布。使用softmax假定这些类使互斥的。这给组合数据集带来了问题，因为上义词和下义词的问题。所以我们使用一个多标签模型来组合不假定互斥的数据集

#### 层次分类

ImageNet的标签使从WordNet中提取的，WordNet使一个语言数据库，用于构造概念及其关系。YOLOv2更具WordNet，将ImageNet和COCO中的名词对象一起构建了一个WordTree，以pysicial object为根节点，各名词以具相互间的关系构建树枝，树叶，节点间的连接表达了对象概念之间蕴含的关系（上位/下位关系）：

![](F:\mycode\knowledgeArrangement\vision\wordTree.jpg)

为了使用WordTree进行分类，我们预测每个节点上的条件概率，即给定那个同义词集合的每个下义词的概率。

如果我们想要计算某个特定节点的绝对概率，只需沿着树的路径到达根节点，然后乘以条件概率。比如：

**P(Norfolk terrier) = P(Norfolk terrier|terrier) \* P(terrier|hunting dog) \* P(hunting dog|dog) \*......\* P(animal|physical object) \* P(physical object)**

令P(physical object) = 1

在训练中，我们沿着树向上传播gt标签，如果图片被标记为“Norfolk terrier”，那么它也会被标记为“dog”和“mammal”。为了计算条件概率，论文预测了一个1369维的向量，并计算所有同义词集合下的softmax，如下：

![1563810679101](F:\mycode\knowledgeArrangement\vision\multilabel.png)

现在在预测时，检测器预测一个边框和概率树。我们沿着树向下遍历，在每次分割时都取最高的置信路径，直到到达某个阈值，然后预测那个对象类。

#### Dataset combination with WordTree

​	我们可以使用WordTree来合理地结合多个数据集，可以简单地将数据集中的类别映射到树中的同义词集。

![1563863805871](F:\mycode\knowledgeArrangement\vision\combining.png)

#### 联合分类与检测

在YOLO9000中，要训练一个超大规模的检测器，所以我们使用COCO检测数据集和来自完整ImageNet发行版的9000个分类创建联合数据集。该数据集对应的WordTree有9418个类。因为ImageNet要比COCO大得多，所以通过过采样（oversampling）COCO使得ImageNet与COCO数据的比值为4：1来平衡数据集。

**在YOLO9000架构中，使用YOLOv2的结构但是只用了3个defualt box priors来限制输出大小。当我们的网络看见检测图片时我们像往常一样反向传播损失。对于分类损失，我们只在标签对应级别或更高级别上反向传播损失，当看到一个分类图像时，只是反向传播分类损失。**

使用联合训练，YOLO9000学会了使用COCO中的检测数据在图像中找到目标，并使用ImageNet中的数据对各种目标进行分类。



当我们分析YOLO9000在ImageNet上的表现时，我们看到他能很好地学习新物种的动物，但却很难学习衣服和设备等种类。

### 总结

YOLOv2在很多检测数据集上都要比其他系统快，此外它还可以运行在各种尺寸图像上并在速度和精度之间提供一个平滑的权衡。而YOLO9000是通过联合优化检测和分类，实现对9000多个目标类别进行检测的实时框架。