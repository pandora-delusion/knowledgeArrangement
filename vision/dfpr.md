# Deep Feature Pyramid Reconfiguration for Object Detection

​	文章认为SSD那种直接在多个特征图上检测目标的方法很难高效的捕捉浅层特征图的深层语义，因为在SSD的那种自底向上的特征图上只能学习较深层的强特征，而不能学习较浅层的强特征。这导致了SSD在检测小目标上的性能瓶颈。

​	为了克服SSD的这一缺点，很多模型致力于将低分辨率强语义的特征和高分辨率弱语义的特征通过自顶向下的线性连接结合起来。这种横向的连接将语义信息逐层传递到浅层，从而提高了浅层特征的检测能力。

​	文章提出，在理想的情况下，卷积网络的特征金字塔应该具有如下特点：

1. 在单个网络的不同层中重复使用多尺度特征
2. 在所有尺度上用强语义改进特征

FPN采用的线性连接特征过于简单以至于很难在更复杂和实际的情况下捕获高维非线性模式。而本文的金字塔结构采用全局注意力机制来强调整张图片的全局信息，然后进行局部重构，在接受域内对局部块进行建模。

​	相较于SSD和FPN， 本文的金字塔结构具有在两个方面有优势：

1. 非线性的局部-全局重构，使特征表现出更强的表达能力
2. 所有尺度下的金字塔处理被同时执行，因此比FPN的逐层转换更加高效

​	

图像金字塔方法是一种非常耗时的方法，因为他需要独立地计算每个图像尺度上的特征，因此无法重用卷积网络的特征。



​	在本文的架构下， 网络包含L层，每层由非线性转换$F_{l}(*)$实现，$l$指代层数。$F_{l}(*)$是一系列的组合变换如卷积，池化，ReLU等等。定义第$l$层的输出为$x_{l}$。网络的整体骨干表示为$X_{net}=\left \{ x_{1},x_{2},...,x_{L} \right \}$。在金字塔架构下，预测特征图集合为$X_{pred}=\left \{ x_{p},x_{p+1},...,x_{L} \right \}$，这里$p \gg 1$。这里的深度特征图$x_{L}$学习了高语义抽象。

​	SSD使用更深的层来检测大目标，而是用浅的和高分辨率层来检测小目标。高分辨率特征图有限的语义信息损害了用来识别目标的表示能力。它们没有机会重用更深的强语义信息来检测小目标。

​	横向连接：为了丰富浅层的语义信息，这种方式添加深层的语义特征。采用FPN的方式，如下：
$$
\begin{align*}
&X_{L}^{'}=X_{L}\\
&X_{L-1}^{'}=\alpha_{L-1} \cdot  X_{L-1}+\beta_{L-1} \cdot X_{L}\\
&X_{L-1}^{'}=\alpha_{L-2} \cdot X_{L-2}+\beta_{L-2} \cdot X_{L-1}^{'}\\
&=\alpha_{L-2}\cdot X_{L-2}+\beta_{L-2} \alpha_{L-1}\cdot X_{L-1}+\beta_{L-2} \beta_{L-1}\cdot X_{L}
\end{align*}
$$
其中$\alpha$，$\beta$是权重。不失一般性：
$$
x_{l}^{'}=\sum_{l=p}^{L}w_{l}\cdot x_{l}
$$
其中$w_{l}$是第$l$层输出的权重，最后用于检测的特征为：
$$
X_{pred}^{'}=\left \{ x_{p}^{'},x_{p+1}^{'},...,x_{L}^{'} \right \}
$$
​	通过上述公式最终特征$x_{l}^{'}$等价于$x_{l},x_{l+1},...,x_{L}$的线性组合。与更深的特征层线性组合是改进指定浅层信息的一种方法，然而用于检测的特征层次结构通常位于非线性特征空间中，因此捕捉这些概念的表示通常是输入高维非线性函数。

​	在本文中，第$l$层的特征生成过程被视为给定特征层次结构的非线性转换：
$$
x_{l}^{'}=H_{l}(X)
$$
​	X是考虑多尺度检测的特征层次结构，为了易于实现，将$H_{l}(X)$的多组输入在转换之前连结成单个tensor

![1568612884226](F:\mycode\knowledgeArrangement\vision\taokong.png)

​	将不同尺寸的特征图连结时需要做自适应采样。由于特征层次的潜在概念的分布没有先验，因此我们需要使用一个通用的函数逼近器来提取每个尺度的特征。该功能还应该保持空间一致性，因为探测器将在相应的位置激活。每个层次的最终特征是特征层次的非线性转换，其中可学习的参数在不同的空间位置之间是共享的。

​	在本文中，特征转换操作$H_{l}( \cdot )$分为全局注意力机制和局部注意力机制，实现为一个轻量级的网络这样可以嵌入到卷积网络中实现端到端的学习。全局和局部操作是互补的，因为它们都在不同尺寸的特征层次结构上处理。

#### 特征层次结构的全局注意力机制

​	全局部分的目的在于在特定尺寸的全局范围内强调有用的信息特征和抑制不太有用的特征，在本文中使用Squeeze-and-Excitation模块。在这里不多讨论，详细见SE模块。

​	原始的SE模块是用来显式地建模通道之间的依赖性，并且在目标检测领域获得了巨大的成功。而在这里，用来强调通道级别的层次结构特征并抑制哪些没有用的特征。

#### 局部重构

​	局部重构网络映射特征层次结构块到输出特征块上，并且在所有局部感受野上共享。本文使用一个residual block作为微型网络的实例，它是一个通用的函数逼近器，可以通过反向传播进行训练

![1568616519509](F:\mycode\knowledgeArrangement\vision\local.png)

最终，局部重构如下：
$$
X_{l}^{'}=R(\widetilde{X}_{t})+W_{l}X_{l}
$$
$R(\cdot)$指的是提高学习的语义的残差块，注意残差学习模块和残差网络的区别，论文的假设是语义信息分布在特征层次结构中间并且残差学习模块可以选择其他信息来优化；另一个区别是本文中残差学习的输入是特征层次结构。

​	因为金字塔的所有层使用共享的特征图，所以文中在所有特征图上固定了特征的维度（即通道数d），文中d为256。残差学习块可以避免目标函数的梯度直接进入主干网络，从而为更好地建模特征层次结构提供了更多的机会。

​	在所有最终的金字塔映射是同时生成的，所以更加高效



