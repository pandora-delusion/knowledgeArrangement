### 自编码器

自编码器内部有一个隐藏层h，可以产生编码（code）表示输入。该网络可以看作两部分组成：一个由函数$h=f(x)$表示的编码器和一个生成重构的解码器$r=g(h)$。不应该将自编码器设计成输入到输出完全相等。这通常需要向自编码器强加一些约束，使它只能近似地复制，并只能复制于训练数据相似的输入。

传统编码器被用于降维或特征学习。

##### 欠完备自编码器
从自编码器获得有用特征的一种方法是限制h的维度比x小，这种编码维度小于输入维度的自编码器称为欠完备自编码器。学习欠完备的表示将强制自编码器捕捉训练数据中最显著的特征。
学习过程可以简单地描述为最小化一个损失函数：
$$L(x,g(f(x)))$$L是损失函数，惩罚$g(f(x))$与x的差异，如均方误差。
当解码器是线性的且L是均方误差，欠完备的自编码器会学习出于PCA相同的生成子空间。这种情况下，自编码器在训练来执行复制任务的同时学到了训练数据的主元子空间。
拥有非线性编码器函数f和非线性解码器函数g的自编码器能够学习出更加强大的PCA非线性推广。但如果编码器和解码器被赋予过大的容量，自编码器会执行复制任务而捕捉不到任何有关数据分布的有用信息。

##### 正则自编码器
理想情况下，根据要建模的数据分布的复杂性，选择合适的编码维度和编码器，解码器容量，就可以成功训练任意架构的自编码器

正则自编码器使用的损失函数可以鼓励模型学习其他特性，而不必限制使用浅层的编码器和解码器以及小的编码维度来限制模型的容量。这些特性包括稀疏表示，表示的小导数以及对噪声或输入缺失的鲁棒性。即使模型容量大到足以学习一个无意义的恒等函数，非线性且过完备的正则自编码器仍然能够从数据中学到一些关于数据分布的有用信息。
1.稀疏自编码器
在训练时结合编码层的稀疏惩罚和重构误差：$$L(x,g(f(x)))+\Omega(h)$$ $h=f(x)$
一般用来学习特征，一边用于像分类这样的任务
我们可以认为这个稀疏网络自编码器框架是对带有潜变量的生成模型的近似最大似然训练，而不将稀疏惩罚是为复制任务的正则化。
（未完待续，看不懂。。。。）