# 深度学习中的结构化概率模型

#####有向模型，也被称为信念网络或者贝叶斯网络
箭头所指方向表示了这个随机变量的概率分布是由其他变量的概率分布所定义的。
变量x的有向概率模型是通过有向无环图G（每个结点都是模型中的随机变量）和一系列的局部条件概率分布$p(x_{i}|P \alpha G(x_{i}))$来定义的，其中$P \alpha G (x_{i})$表示节点$x_{i}$的所有父节点。x的概率分布可以表示为：
$$p(x)= \prod p(x_{i} | P \alpha G(x_{i}) )$$
相比于非结构化建模减少了很多参数

##### 无向模型
也被称为马尔可夫随机场或者马尔可夫网络
当相互的作用并没有本质性的只想，或者明确的双向交互作用时，使用无向模型更加合适
如果在无向模型中的两个节点通过一条边相连接，那么对应这些节点的随机变量之间是直接作用的。不同于有向模型。在无向模型中的边是没有方向的，并不与一个条件分布相关联。

正式的说，一个无向模型是一个定义在无向模型G上的结构化概率模型。对于图中的每一个团C（图中的一个团是图中节点的一个子集，并且其中的点是全连接的），一个因子$\phi (C)$（也成为团势能），衡量了团中变量每一种可能的联合状态所对应的密切程度。这些因子都被限制为是非负的。它们一起定义了未归一化概率函数：
$$\widetilde{p}(x)=\prod_{C\in G}\phi (C)$$

##### 配分函数
为了保证概率之和或者积分为1，为了得到有效的概率分布，我们需要使用对应的归一化概率分布：$$p(x)=\frac{1}{Z} \widetilde{p}(x)$$ Z是使得所有的概率之和或者积分为1 的常数，满足：$$Z=\int \widetilde{p}\left ( x \right )dx$$ Z很难计算。

在设计无向模型时，设定一些使得Z不存在的因子也是有可能的。当模型中的一些变量是连续的，且$\widetilde{p}$在其定义域上的积分发散时这种情况就会发生。由于积分式发散的，随意不存在一个对应这个势能函数$\phi (x)$的概率分布。有时候，$\phi $函数某些参数的选择可以决定相应的概率分布是否能够被定义。

每一个变量的定义域对于一系列给定的$\phi$函数所对应的概率分布有着重要的影响。

##### 基于能量的模型
无向模型中许多理论结果都依赖于$\forall x,\widetilde{p}\left ( x \right )> 0$这个假设。使这个条件满足的一种简单方式是使用基于能量的模型（Energy-based model, EBM），其中：$$\widetilde{p}\left ( x \right )=exp\left ( -E\left ( x \right ) \right )$$ E(x)被称为能量函数。这样可以自由选择能够简化学习过程的能量函数。
服从上式的任意分布都是玻尔兹曼分布的一个实例，我们把许多基于能量的模型称为玻尔兹曼机。
无向模型中的团对应于过归一化概率函数中的因子。通过$exp(a+b)=exp(a)exp(b)$，无向模型中的不同团对应于能量函数的不同项。能量函数中的每一项对应的是概率分布中的一个因子。
许多对概率模型进行操作的算法不需要计算$P_{model}(x)$，而只需要计算$log \widetilde{p}_{model}(x)$。对于具有潜变量h的基于能量的模型，这些算法有时会将该量的复数称为自由能：$$F(x)=-log \sum_{h} exp(-E(x,h))$$ 在这本书中，我们更倾向于更为通用的基于$log \widetilde{P}(x)$

##### 分离和d-分离
图模型中的边告诉我们哪些变量直接互相作用。我们经常需要知道哪些变量间接相互作用。某些间接相互作用可以通过观察其他变量来启用或者禁用。

在无向模型中，图中隐含的条件独立性称为分离。如果图结构显示给定变量集S的情况下变量集A和变量集B无关，那我们声称给定变量集S时，变量集A与另一组变量集B时分离的。如果连接两个变量a和b的连接路径仅涉及未观察变量，那么这些变量不是分离的。如果它们之间没有路径，或者所有路径都包含可观测的变量，那么它们时分离的。我们认为仅涉及未观察到的变量的路径是"活跃"的，而包括可观察变量的路径称为"非活跃"的。

在有向模型中，这些概念被称为d-分离。"d"代表依赖的意思，有向图中d-分离的定义与无向模型中分离的定义相同：如果图结构显示给定变量集S时，变量集A与变量集B无关，那么我们认为给定变量集S时，变量集A d-分离于变量集B。我们可以通过查看图中存在的活跃路径来检查图中隐含的独立性。如果两个变量之间存在活跃路径，则两个变量是依赖的。如果没有活跃路径，则为d-分离。在有向网络中，确定路径是否活跃比较复杂。
[相消解释作用](https://martin-thoma.com/explaining-away/): 阻塞V-结构中路径的唯一方法就是共享子节点的后代一个都观察不到

##### 在有向模型和无向模型中转换
没有概率模型本质上是无向或有向的。根据哪种方法可以最大程度地捕捉到概率分布中的独立性，或者那种方法使用最少的边来描述分布，我们可以决定使用有向建模还是无向建模。

有向模型通常提供了一种高效地从模型中抽取样本的直接方法。而无向模型形式通常对于推导近似推断过程是很有用的。

有向模型能够使用一种无向模型无法完美表示的特性类型的子结构。这个子结构被称为"不道德(immorality)"。这种结构出现在两个随机变量a和b都是第三个随机变量c的父节点，并且不存在任一方向上直接连接a和b的边时。为了将有向模型图D转换为无向模型，我们需要创建一个新图U。对于每队变量x和y，如果存在连接D中的x和y的有向边（任一方向上），或者如果x和y都是图D中另一个变量z的父节点，在U中添加连接x和y的无向边。得到的图U被称为道德图（moralized graph）。 不道德结构的有向图模型转化为无向图模型会丢失独立性。道德化的过程会给图添加许多边，因此丢失了一些隐含的独立性。

无向模型可以包含无向模型不能完美表示的子结构。如果U包含长度大于3的环，则有向图D不能捕获五项模型U所包含的所有条件独立性，除非该环还包含弦（弦定义环序列中任意两个非连续变量之间的连接）。如果U具有长度为4或更大的环，并且这些环没有弦，我们必须在将它们转换为有向模型之前添加弦。添加弦会丢弃在U中编码的一些独立信息。通过将弦添加到U形成的图被称为弦图或者三角形化图，因为我们现在可以用更小的、三角的环来描述所有的环。要从弦图中构建有向图D，我们还需要为边指定方向。当这样做时，我们不能在D中创建有向环，否则无法定义有效的有向概率模型。为D中的边分配方向的一种方法是对随机变量排序，然后将每个边从排序较早的节点指向排序稍后的节点。

##### 因子图
从无向模型中抽样的另一种方法，他可以解决标准无向模型语法中图表达的模糊性。因子图是一个包含无向二分图的无向模型的图形化表示。圆形节点对应于随机变量，方块节点对应于未归一化概率函数的因子$\phi$。变量和因子可以通过无向边连接，当且仅当变量包含在未归一化概率函数的因子中时，变量和因子在图中存在连接。没有因子可以连接到图中的另一个因子，也不能将变量连接到变量。

### 从图模型中采样
图模型简化了从模型中采样的过程。
有向图模型的一个优点是：可以通过一个简单高效的过程从模型所表示的联合分布中产生样本，这个过程被称为原始采样(ancestral sampling)。
原始采样的基本思想是将图中的变量$x_{i}$使用拓扑排序，使得对于所有的i和j，如果$x_{i}$是$x_{j}$的一个父亲结点，这则j大于i。然后可以按此顺序对变量进行采样。只要不难从每个条件分布$x_{i}\sim P\left ( x_{i}|PaG\left ( x_{i} \right ) \right )$中采样。那么从整个模型中采样也是容易的。
有些图可能存在多个拓扑排序。原始采样可以使用这些拓扑排序中的任何一个。
原始采样通常非常快，并且非常简便。
原始采样仅适用于有向图模型，它并不是每次采样都是条件采样操作。当我们希望从有向模型中变量的子集中采样时，给定一些其他变量，我们经常要求所有给定的条件变量在顺序图中要比采样的变量顺序要早。在这种情况下，我们可以从模型分布指定的局部条件概率中采样。否则，我们需要采样的条件分布是给定观测变量的后验分布。这些后验分布在模型中通常没有明确指定和参数化。推断这种后验分布的代价可能是很高的。在这种情况下的模型中，原始采样不再生效。
可以将无向模型转换为有向模型来实现从无向模型中采样，但是推断问题非常棘手（要确定新有向图的根节点上的边缘分布），或者需要引入许多边，从而使得得到的有向模模型变得难处理。
从无向模型中采样，而不首先将其转换为有向模型的做法似乎需要解决循环依赖的问题。
从无向模型中抽取样本是一个成本很高的多次迭代过程。我们很难确定样本何时达到所期望分布的足够精确的近似。

### 结构化建模的优势
使用结构化概率模型的主要优点是：显著降低表示概率分布、学习和推断的成本。有向模型中采样还可以被加速，但是对于无向模型情况会比较复杂。结构化概率模型允许我们明确地将给定的现有知识于知识的学习或者推断分开，这使我们的模型更加容易开发和调试。

### 学习和依赖关系
良好的生成模型需要准确地捕获可见变量v上的分布。通常v的不同元素彼此高度依赖。在深度学习中，最常用于建模这些依赖关系的方法是引入几个潜在或"隐藏"变量h。然后，该模型可以捕获任何对（变量$v_{i}$和$v_{j}$间接依赖可以通过$v_{i}$和h之间直接依赖，h和$v_{j}$直接依赖捕获）之间的依赖关系。
[结构学习]????

### 推断和近似推断
解决变量之间如何相互关联的问题是我们使用概率模型的一个主要方式。
图结构允许我们使用合理的参数来表示复杂的高维分布，但是用于深度学习的图不满足这样的条件，从而难以实现高效的推断。

### 结构化概率模型的深度学习方法
如果从潜变量$h_{i}$到可观察变量的最短路径是j步，我们可以认为潜变量$h_{i}$处于深度j。我们通常将模型的深度描述为任何这样的$h_{i}$的最大深度（和计算图定义的深度不同）。